{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90861ecd",
   "metadata": {},
   "source": [
    "\n",
    "# IFN580 â€“ Assignment 1: Starter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8812c4",
   "metadata": {},
   "source": [
    "## 1) Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- User config ----\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COL   = \"IsBadBuy\"   # change if different\n",
    "DATA_PATHS   = [\n",
    "    \"kick.csv\",                         # put the CSV next to this notebook\n",
    "    \"assignment 1 data kick.csv\",       # alt name (rename as needed)\n",
    "    \"./data/kick.csv\",                  # common project structure\n",
    "]\n",
    "\n",
    "# ---- Imports ----\n",
    "import os, sys, math, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,\n",
    "                             roc_curve, auc, RocCurveDisplay, classification_report,\n",
    "                             accuracy_score)\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectFromModel\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa679fd1",
   "metadata": {},
   "source": [
    "## 2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d25247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_data(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "DATA_PATH = find_data(DATA_PATHS)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"CSV not found. Place your kick dataset next to this notebook as 'kick.csv' \"\n",
    "        f\"or update DATA_PATHS.\"\n",
    "    )\n",
    "\n",
    "print(f\"Using data at: {DATA_PATH}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "# --- PATCH AFTER \"2) Load Data\": handle '?' and coerce numeric-like columns ---\n",
    "# Replace '?' with real missing values so the pipeline can impute correctly\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Try to convert columns that look numeric into real numbers\n",
    "coerced = []\n",
    "for c in df.columns:\n",
    "    if c == \"IsBadBuy\":\n",
    "        continue\n",
    "    s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    # Treat as numeric if most values can be converted\n",
    "    if (s.notna().mean() > 0.6) and (s.notna().sum() > 100):\n",
    "        df[c] = s\n",
    "        coerced.append(c)\n",
    "print(\"Coerced to numeric (first few):\", coerced[:10], \"| total:\", len(coerced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b90fed",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Quick Audit (Task 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcafe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic info\n",
    "display(df.info())\n",
    "display(df.describe(include='all').T)\n",
    "\n",
    "# Target distribution (before preprocessing)\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise KeyError(f\"TARGET_COL '{TARGET_COL}' not found. Set TARGET_COL correctly.\")\n",
    "\n",
    "target_counts = df[TARGET_COL].value_counts(dropna=False)\n",
    "target_ratio  = target_counts / len(df)\n",
    "print(\"Target counts (before):\")\n",
    "display(pd.DataFrame({\"count\": target_counts, \"ratio\": target_ratio}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2759410",
   "metadata": {},
   "source": [
    "## 4) Train/Test Split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac011aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"IsBadBuy\"  # change if your target name differs\n",
    "\n",
    "# Drop pure identifiers and raw text date (we already have PurchaseTimestamp)\n",
    "drop_cols = [c for c in [\"PurchaseID\", \"PurchaseDate\"] if c in df.columns]\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[TARGET_COL] + drop_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(\"Train/Test sizes:\", X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830eb4b7",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Preprocessing Pipeline (ColumnTransformer)\n",
    "- **Numeric**: median imputation + (optional) scaling\n",
    "- **Categorical**: mode imputation + OneHotEncoder(handle_unknown=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from packaging import version\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.columns.difference(numeric_features).tolist()\n",
    "\n",
    "# Numeric: impute median then standardize\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())  # dense pipeline\n",
    "])\n",
    "\n",
    "# Categorical: impute mode then OHE (compatible with new sklearn)\n",
    "ohe_kwargs = dict(handle_unknown=\"ignore\")\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kwargs[\"sparse_output\"] = False   # new param name in newer sklearn\n",
    "else:\n",
    "    ohe_kwargs[\"sparse\"] = False          # legacy param name for older sklearn\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "print(\"Numeric features:\", len(numeric_features))\n",
    "print(\"Categorical features:\", len(categorical_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab65a9",
   "metadata": {},
   "source": [
    "## 6) Helpers: ROC / Confusion Matrix / Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc, RocCurveDisplay,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    accuracy_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_scores(model, X):\n",
    "    \"\"\"Return a 1-D score/probability array for ROC/PR curves.\"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return model.decision_function(X)\n",
    "    raise ValueError(\"Model has neither predict_proba nor decision_function.\")\n",
    "\n",
    "def plot_roc(model, X_test, y_test, title=None):\n",
    "    \"\"\"Plot ROC curve and return AUC.\"\"\"\n",
    "    y_score = get_scores(model, X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                    estimator_name=title or type(model).__name__).plot()\n",
    "    plt.title((title or \"ROC\") + f\" | AUC={roc_auc:.3f}\")\n",
    "    plt.show()\n",
    "    return roc_auc, y_score\n",
    "\n",
    "def plot_cm(model, X_test, y_test, title=None):\n",
    "    \"\"\"Plot confusion matrix at default threshold 0.5 (or model's default).\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm).plot(ax=ax)\n",
    "    ax.set_title(title or \"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    return cm\n",
    "\n",
    "def plot_pr(model, X_test, y_test, title=None):\n",
    "    \"\"\"Plot Precision-Recall curve (useful for imbalanced data).\"\"\"\n",
    "    y_score = get_scores(model, X_test)\n",
    "    p, r, _ = precision_recall_curve(y_test, y_score)\n",
    "    ap = average_precision_score(y_test, y_score)\n",
    "    plt.figure()\n",
    "    plt.plot(r, p)\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title((title or \"Precision-Recall\") + f\" | AP={ap:.3f}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    return ap, y_score\n",
    "    \n",
    "def summary_scores(model, X_tr, y_tr, X_te, y_te):\n",
    "    \"\"\"Return a tiny dict of train/test accuracy for quick reporting.\"\"\"\n",
    "    yhat_tr = model.predict(X_tr)\n",
    "    yhat_te = model.predict(X_te)\n",
    "    return {\n",
    "        \"train_acc\": accuracy_score(y_tr, yhat_tr),\n",
    "        \"test_acc\":  accuracy_score(y_te, yhat_te),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67acf02e",
   "metadata": {},
   "source": [
    "## 7) Baselines: Decision Tree & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt_clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "dt_clf.fit(X_train, y_train)\n",
    "dt_scores = summary_scores(dt_clf, X_train, y_train, X_test, y_test)\n",
    "dt_auc = plot_roc(dt_clf, X_test, y_test, title=\"ROC â€“ DecisionTree (baseline)\")\n",
    "_ = plot_cm(dt_clf, X_test, y_test, title=\"CM â€“ DecisionTree (baseline)\")\n",
    "print(\"DT (baseline) scores:\", dt_scores, \"AUC:\", dt_auc)\n",
    "\n",
    "log_clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, solver=\"lbfgs\"))\n",
    "])\n",
    "log_clf.fit(X_train, y_train)\n",
    "log_scores = summary_scores(log_clf, X_train, y_train, X_test, y_test)\n",
    "log_auc = plot_roc(log_clf, X_test, y_test, title=\"ROC â€“ LogisticRegression (baseline)\")\n",
    "_ = plot_cm(log_clf, X_test, y_test, title=\"CM â€“ LogisticRegression (baseline)\")\n",
    "print(\"LogReg (baseline) scores:\", log_scores, \"AUC:\", log_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d49af",
   "metadata": {},
   "source": [
    "## 8) GridSearchCV Stubs (DT / Logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Decision Tree grid\n",
    "dt_grid = {\n",
    "    \"clf__max_depth\": [None, 5, 8, 12],\n",
    "    \"clf__min_samples_split\": [2, 10, 50],\n",
    "    \"clf__min_samples_leaf\": [1, 5, 20]\n",
    "}\n",
    "dt_pipe = Pipeline([(\"prep\", preprocess),\n",
    "                    (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))])\n",
    "dt_gs = GridSearchCV(dt_pipe, dt_grid, cv=cv, n_jobs=-1, scoring=\"roc_auc\")\n",
    "dt_gs.fit(X_train, y_train)\n",
    "print(\"DT best params:\", dt_gs.best_params_, \"best AUC:\", dt_gs.best_score_)\n",
    "dt_best = dt_gs.best_estimator_\n",
    "_ = plot_roc(dt_best, X_test, y_test, title=\"ROC â€“ DecisionTree (tuned)\")\n",
    "_ = plot_cm(dt_best, X_test, y_test, title=\"CM â€“ DecisionTree (tuned)\")\n",
    "\n",
    "# Logistic grid (L2 regularisation)\n",
    "log_grid = {\n",
    "    \"clf__C\": np.logspace(-6, 3, 10),\n",
    "    \"clf__penalty\": [\"l2\"]\n",
    "}\n",
    "log_pipe = Pipeline([(\"prep\", preprocess),\n",
    "                     (\"clf\", LogisticRegression(max_iter=5000, solver=\"lbfgs\"))])\n",
    "log_gs = GridSearchCV(log_pipe, log_grid, cv=cv, n_jobs=-1, scoring=\"roc_auc\")\n",
    "log_gs.fit(X_train, y_train)\n",
    "print(\"LogReg best params:\", log_gs.best_params_, \"best AUC:\", log_gs.best_score_)\n",
    "log_best = log_gs.best_estimator_\n",
    "_ = plot_roc(log_best, X_test, y_test, title=\"ROC â€“ LogisticRegression (tuned)\")\n",
    "_ = plot_cm(log_best, X_test, y_test, title=\"CM â€“ LogisticRegression (tuned)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c844f2",
   "metadata": {},
   "source": [
    "## 9) Feature Selection (RFE / SelectFromModel via DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2873690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Refit preprocess on full training to get feature names after OHE\n",
    "prep_only = preprocess.fit(X_train)\n",
    "feature_names = []\n",
    "# Obtain transformed feature names\n",
    "try:\n",
    "    feature_names = prep_only.get_feature_names_out().tolist()\n",
    "except Exception as e:\n",
    "    print(\"Could not extract feature names from ColumnTransformer:\", e)\n",
    "\n",
    "# --- RFE with Logistic ---\n",
    "# We do RFE on a *dense* matrix; transform X_train -> matrix first\n",
    "Xtr_mat = prep_only.transform(X_train)\n",
    "Xte_mat = prep_only.transform(X_test)\n",
    "\n",
    "base_log = LogisticRegression(max_iter=5000, solver=\"lbfgs\")\n",
    "rfe = RFECV(base_log, step=1, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "rfe.fit(Xtr_mat, y_train)\n",
    "\n",
    "selected_mask = rfe.support_\n",
    "if feature_names and len(feature_names) == len(selected_mask):\n",
    "    selected_features = [f for f, keep in zip(feature_names, selected_mask) if keep]\n",
    "    print(f\"RFE kept {len(selected_features)} features\")\n",
    "else:\n",
    "    selected_features = np.where(selected_mask)[0].tolist()\n",
    "    print(f\"RFE kept {len(selected_features)} (index positions; names unavailable)\")\n",
    "\n",
    "# Train Logistic on RFE-selected features\n",
    "base_log.fit(Xtr_mat[:, selected_mask], y_train)\n",
    "y_score = base_log.predict_proba(Xte_mat[:, selected_mask])[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "plt.figure()\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc(fpr, tpr),\n",
    "                estimator_name=\"LogReg (RFE)\").plot()\n",
    "plt.title(\"ROC â€“ Logistic (RFE-selected)\")\n",
    "plt.show()\n",
    "\n",
    "# --- SelectFromModel using DT (from tuned DT) ---\n",
    "dt_selector = SelectFromModel(dt_best[\"clf\"], prefit=True, threshold=\"median\")\n",
    "# transform on features *after* preprocess\n",
    "# need to rerun dt_best['prep'] to get same mapping\n",
    "Xtr_dt = dt_best[\"prep\"].transform(X_train)\n",
    "Xte_dt = dt_best[\"prep\"].transform(X_test)\n",
    "Xtr_sel = dt_selector.transform(Xtr_dt)\n",
    "Xte_sel = dt_selector.transform(Xte_dt)\n",
    "\n",
    "# Fit logistic on DT-selected features\n",
    "log_sel = LogisticRegression(max_iter=5000, solver=\"lbfgs\")\n",
    "log_sel.fit(Xtr_sel, y_train)\n",
    "y_score = log_sel.predict_proba(Xte_sel)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "plt.figure()\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc(fpr, tpr),\n",
    "                estimator_name=\"LogReg (DT-selected)\").plot()\n",
    "plt.title(\"ROC â€“ Logistic (DT-selected)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe4338",
   "metadata": {},
   "source": [
    "## 10) Neural Networks (MLPClassifier) â€“ full & reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6197459",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grid stubs (small first)\n",
    "mlp_grid = {\n",
    "    \"clf__hidden_layer_sizes\": [(3,), (5,), (7,), (9,)],\n",
    "    \"clf__alpha\": [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "}\n",
    "\n",
    "mlp_pipe = Pipeline([(\"prep\", preprocess),\n",
    "                     (\"clf\", MLPClassifier(random_state=RANDOM_STATE, max_iter=500))])\n",
    "\n",
    "mlp_gs = GridSearchCV(mlp_pipe, mlp_grid, cv=cv, n_jobs=-1, scoring=\"roc_auc\")\n",
    "mlp_gs.fit(X_train, y_train)\n",
    "print(\"MLP best params:\", mlp_gs.best_params_, \"best AUC:\", mlp_gs.best_score_)\n",
    "mlp_best = mlp_gs.best_estimator_\n",
    "_ = plot_roc(mlp_best, X_test, y_test, title=\"ROC â€“ MLP (tuned, full features)\")\n",
    "_ = plot_cm(mlp_best, X_test, y_test, title=\"CM â€“ MLP (tuned, full features)\")\n",
    "\n",
    "# Reduced features with DT-selected mask\n",
    "# Reuse Xtr_sel/Xte_sel from DT-select above\n",
    "mlp_reduced = MLPClassifier(random_state=RANDOM_STATE, max_iter=500)\n",
    "mlp_reduced.fit(Xtr_sel, y_train)\n",
    "y_score = mlp_reduced.predict_proba(Xte_sel)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "plt.figure()\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc(fpr, tpr),\n",
    "                estimator_name=\"MLP (DT-selected)\").plot()\n",
    "plt.title(\"ROC â€“ MLP (DT-selected features)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1b4cf",
   "metadata": {},
   "source": [
    "## 11) Final comparisons (ROC winners only â€“ prepare for Task 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winners: update these to the tuned/best models you actually want to compare\n",
    "winners = [\n",
    "    (\"DecisionTree (tuned)\", dt_best),\n",
    "    (\"Logistic (tuned)\",     log_best),\n",
    "    (\"MLP (tuned full)\",     mlp_best),\n",
    "]\n",
    "\n",
    "# 1) Sanity print: confirm different estimators and params\n",
    "for name, model in winners:\n",
    "    est = model.named_steps[\"clf\"]\n",
    "    print(f\"{name}: {type(est).__name__} | params={getattr(est, 'get_params', lambda: {})()}\")\n",
    "\n",
    "# 2) Overlay ROC curves in ONE figure with labels/AUC\n",
    "plt.figure()\n",
    "scores = {}\n",
    "for name, model in winners:\n",
    "    y_score = get_scores(model, X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    scores[name] = y_score\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Comparison â€“ Winners\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 3) (Optional) Correlation check of score vectors: if ~1.0 then curves will look identical\n",
    "import numpy as np\n",
    "names = list(scores.keys())\n",
    "for i in range(len(names)):\n",
    "    for j in range(i+1, len(names)):\n",
    "        r = np.corrcoef(scores[names[i]], scores[names[j]])[0,1]\n",
    "        print(f\"score corr({names[i]} vs {names[j]}) = {r:.4f}\")\n",
    "\n",
    "# 4) (Optional) PR curves â€“ more informative with class imbalance\n",
    "plt.figure()\n",
    "for name, model in winners:\n",
    "    y_score = scores[name]\n",
    "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "    p, r, _ = precision_recall_curve(y_test, y_score)\n",
    "    ap = average_precision_score(y_test, y_score)\n",
    "    plt.plot(r, p, label=f\"{name} (AP={ap:.3f})\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precisionâ€“Recall Comparison â€“ Winners\")\n",
    "plt.legend(loc=\"lower left\"); plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4801467f",
   "metadata": {},
   "source": [
    "## 12) Helpers to save artifacts (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5568447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "def save_scores_table(models, X_tr, y_tr, X_te, y_te, path=\"figures/scores.csv\"):\n",
    "    rows = []\n",
    "    for name, model in models:\n",
    "        sc = summary_scores(model, X_tr, y_tr, X_te, y_te)\n",
    "        rows.append({\"model\": name, **sc})\n",
    "    pd.DataFrame(rows).to_csv(path, index=False)\n",
    "    print(f\"Saved {path}\")\n",
    "\n",
    "# Example usage:\n",
    "# save_scores_table(winners, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff62fa0b-7259-4617-b7dc-8cde58c03388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_current_figure(path):\n",
    "    \"\"\"Save the current Matplotlib figure to 'figures/path'.\"\"\"\n",
    "    os.makedirs(\"figures\", exist_ok=True)\n",
    "    plt.gcf().savefig(os.path.join(\"figures\", path), dpi=150, bbox_inches=\"tight\")\n",
    "    print(\"Saved figure ->\", os.path.join(\"figures\", path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85189987-3ec1-44be-a1fa-105b06e53dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- call save_current_figure(\"name.png\") after each plot ---\n",
    "import os, matplotlib.pyplot as plt\n",
    "def save_current_figure(path):\n",
    "    os.makedirs(\"figures\", exist_ok=True)\n",
    "    plt.gcf().savefig(os.path.join(\"figures\", path), dpi=150, bbox_inches=\"tight\")\n",
    "    print(\"Saved ->\", os.path.join(\"figures\", path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b454165-eb56-4c2c-9d08-4b3666e9f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QUICK HEALTH CHECK ---\n",
    "import numpy as np, pandas as pd, sklearn\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "\n",
    "# 1) Basic shapes\n",
    "print(\"X_train/X_test shapes:\", X_train.shape, X_test.shape)  # expect ~ (33180, ?), (8296, ?)\n",
    "\n",
    "# 2) Target ratio stays similar after split (stratified)\n",
    "overall = y.value_counts(normalize=True).to_dict()\n",
    "testset = y_test.value_counts(normalize=True).to_dict()\n",
    "print(\"IsBadBuy ratio overall:\", overall)\n",
    "print(\"IsBadBuy ratio testset:\", testset)\n",
    "\n",
    "# 3) No '?' left (we replaced with NaN)\n",
    "q_left = int((df == '?').sum().sum())\n",
    "print(\"Remaining '?' cells:\", q_left, \"->\", \"OK\" if q_left == 0 else \"CHECK\")\n",
    "\n",
    "# 4) Feature types discovered by the pipeline step\n",
    "print(\"Numeric features:\", len(X_train.select_dtypes(np.number).columns))\n",
    "print(\"Categorical features:\", len(X_train.select_dtypes('object').columns))\n",
    "\n",
    "# 5) Best models exist (means GridSearch cells ran)\n",
    "print(\"Has dt_best:\", 'dt_best' in globals())\n",
    "print(\"Has log_best:\", 'log_best' in globals())\n",
    "print(\"Has mlp_best:\", 'mlp_best' in globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07172214-0815-44e7-b581-44ee9b173094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, sklearn, os\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "def ok(msg, cond):\n",
    "    print((\"âœ“ \" if cond else \"âœ— \") + msg)\n",
    "    return bool(cond)\n",
    "\n",
    "def get_scores(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return model.decision_function(X)\n",
    "    raise ValueError(\"Model has neither predict_proba nor decision_function.\")\n",
    "\n",
    "print(\"sklearn version:\", sklearn.__version__)\n",
    "print(\"--- QUICK QA ---\")\n",
    "\n",
    "# 1) Data shape (expected ~41476 rows; test ~0.2)\n",
    "ok(\"DataFrame exists\", 'df' in globals())\n",
    "rows_ok = 'df' in globals() and (len(df) > 40000)  # allow small variations\n",
    "ok(f\"Rows >= 40000 (got {len(df) if 'df' in globals() else 'N/A'})\", rows_ok)\n",
    "\n",
    "# 2) Sentinels handled\n",
    "q_left = int(((df == '?').sum().sum()) if 'df' in globals() else -1)\n",
    "ok(f\"No '?' remaining (found {q_left})\", q_left == 0)\n",
    "\n",
    "# 3) Split sanity (sizes + stratify)\n",
    "splits_ok = all(v in globals() for v in [\"X_train\",\"X_test\",\"y_train\",\"y_test\"])\n",
    "ok(\"Train/Test defined\", splits_ok)\n",
    "if splits_ok:\n",
    "    print(\"X_train/X_test:\", X_train.shape, X_test.shape)\n",
    "    # expected test rows â‰ˆ 0.2 * 41476 â‰ˆ 8296\n",
    "    ok(\"Test size reasonable (~20%)\", 0.18 <= (len(X_test)/ (len(X_test)+len(X_train))) <= 0.22)\n",
    "    # stratify check (tolerance 2%)\n",
    "    ov = y.value_counts(normalize=True).to_dict() if 'y' in globals() else {}\n",
    "    ts = y_test.value_counts(normalize=True).to_dict()\n",
    "    drift = max(abs(ov.get(k,0)-ts.get(k,0)) for k in ts.keys())\n",
    "    ok(f\"Stratification drift < 2% (Î”={drift:.3f})\", drift < 0.02)\n",
    "\n",
    "# 4) Pipeline & dtypes\n",
    "pipe_ok = 'preprocess' in globals()\n",
    "ok(\"Preprocessing pipeline exists\", pipe_ok)\n",
    "if pipe_ok:\n",
    "    # OneHotEncoder outputs dense?\n",
    "    try:\n",
    "        ohe = preprocess.named_transformers_['cat'].named_steps['onehot']\n",
    "        params = ohe.get_params()\n",
    "        dense = ((\"sparse_output\" in params and params[\"sparse_output\"] is False) or\n",
    "                 (\"sparse\" in params and params[\"sparse\"] is False))\n",
    "        ok(\"OneHotEncoder outputs dense\", dense)\n",
    "    except Exception as e:\n",
    "        ok(\"OneHotEncoder check\", False); print(\"  note:\", e)\n",
    "# Numeric features should be more than 4 after coercion\n",
    "num_cnt = len(X_train.select_dtypes(np.number).columns) if splits_ok else 0\n",
    "ok(f\"Numeric features > 4 (got {num_cnt})\", num_cnt > 4)\n",
    "\n",
    "# 5) Baseline models trained?\n",
    "base_ok = True\n",
    "for name in [\"dt_clf\",\"log_clf\"]:\n",
    "    has = name in globals()\n",
    "    base_ok &= has\n",
    "    ok(f\"Baseline '{name}' exists\", has)\n",
    "\n",
    "# 6) Tuned models exist (GridSearch ran)?\n",
    "tuned_ok = True\n",
    "for name in [\"dt_best\",\"log_best\",\"mlp_best\"]:\n",
    "    has = name in globals()\n",
    "    tuned_ok &= has\n",
    "    ok(f\"Tuned '{name}' exists\", has)\n",
    "\n",
    "# 7) Compute quick AUCs (if models exist)\n",
    "def quick_auc(label, model):\n",
    "    try:\n",
    "        y_score = get_scores(model, X_test)\n",
    "        auc_val = roc_auc_score(y_test, y_score)\n",
    "        print(f\"  {label} AUC(test) = {auc_val:.3f}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  {label} AUC FAILED:\", e)\n",
    "        return False\n",
    "\n",
    "if splits_ok:\n",
    "    if 'dt_clf' in globals(): quick_auc(\"DT baseline\", dt_clf)\n",
    "    if 'log_clf' in globals(): quick_auc(\"LogReg baseline\", log_clf)\n",
    "    if 'dt_best' in globals(): quick_auc(\"DT tuned\", dt_best)\n",
    "    if 'log_best' in globals(): quick_auc(\"LogReg tuned\", log_best)\n",
    "    if 'mlp_best' in globals(): quick_auc(\"MLP tuned\", mlp_best)\n",
    "\n",
    "# 8) Winners overlay sanity: three distinct estimators?\n",
    "if tuned_ok:\n",
    "    winners = [(\"DT (tuned)\", dt_best), (\"LogReg (tuned)\", log_best), (\"MLP (tuned)\", mlp_best)]\n",
    "    try:\n",
    "        est_names = [type(m.named_steps[\"clf\"]).__name__ for _, m in winners]\n",
    "        ok(f\"Distinct estimators for winners: {est_names}\", len(set(est_names)) == 3)\n",
    "    except Exception as e:\n",
    "        ok(\"Winners distinctness check\", False); print(\"  note:\", e)\n",
    "\n",
    "print(\"-\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

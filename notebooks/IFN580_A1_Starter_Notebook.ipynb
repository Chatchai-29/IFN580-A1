{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90861ecd",
   "metadata": {},
   "source": [
    "\n",
    "# IFN580 â€“ Assignment 1: Starter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8812c4",
   "metadata": {},
   "source": [
    "## 1) Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441b2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- User config ----\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COL   = \"IsBadBuy\"   # change if different\n",
    "DATA_PATHS   = [\n",
    "    \"kick.csv\",                         # put the CSV next to this notebook\n",
    "    \"assignment 1 data kick.csv\",       # alt name (rename as needed)\n",
    "    \"./data/kick.csv\",                  # common project structure\n",
    "]\n",
    "\n",
    "# ---- Imports ----\n",
    "import os, sys, math, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,\n",
    "                             roc_curve, auc, RocCurveDisplay, classification_report,\n",
    "                             accuracy_score)\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectFromModel\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa679fd1",
   "metadata": {},
   "source": [
    "## 2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d25247",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "CSV not found. Place your kick dataset next to this notebook as 'kick.csv' or update DATA_PATHS.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m DATA_PATH \u001b[38;5;241m=\u001b[39m find_data(DATA_PATHS)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DATA_PATH \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV not found. Place your kick dataset next to this notebook as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkick.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor update DATA_PATHS.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing data at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DATA_PATH)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: CSV not found. Place your kick dataset next to this notebook as 'kick.csv' or update DATA_PATHS."
     ]
    }
   ],
   "source": [
    "\n",
    "def find_data(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "DATA_PATH = find_data(DATA_PATHS)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"CSV not found. Place your kick dataset next to this notebook as 'kick.csv' \"\n",
    "        f\"or update DATA_PATHS.\"\n",
    "    )\n",
    "\n",
    "print(f\"Using data at: {DATA_PATH}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "# --- PATCH AFTER \"2) Load Data\": handle '?' and coerce numeric-like columns ---\n",
    "# Replace '?' with real missing values so the pipeline can impute correctly\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Try to convert columns that look numeric into real numbers\n",
    "coerced = []\n",
    "for c in df.columns:\n",
    "    if c == \"IsBadBuy\":\n",
    "        continue\n",
    "    s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    # Treat as numeric if most values can be converted\n",
    "    if (s.notna().mean() > 0.6) and (s.notna().sum() > 100):\n",
    "        df[c] = s\n",
    "        coerced.append(c)\n",
    "print(\"Coerced to numeric (first few):\", coerced[:10], \"| total:\", len(coerced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b90fed",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Quick Audit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcafe21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Basic info\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m display(df\u001b[38;5;241m.\u001b[39minfo())\n\u001b[0;32m      3\u001b[0m display(df\u001b[38;5;241m.\u001b[39mdescribe(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Target distribution (before preprocessing)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic info\n",
    "display(df.info())\n",
    "display(df.describe(include='all').T)\n",
    "\n",
    "# Target distribution (before preprocessing)\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise KeyError(f\"TARGET_COL '{TARGET_COL}' not found. Set TARGET_COL correctly.\")\n",
    "\n",
    "target_counts = df[TARGET_COL].value_counts(dropna=False)\n",
    "target_ratio  = target_counts / len(df)\n",
    "print(\"Target counts (before):\")\n",
    "display(pd.DataFrame({\"count\": target_counts, \"ratio\": target_ratio}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648b0d6-7001-46c6-8bfb-19a334b6aea2",
   "metadata": {},
   "source": [
    "## 4)Train/Test Split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ce7759-dfdf-48e8-982e-343729f1a582",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m TARGET_COL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIsBadBuy\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# change if your target name differs\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Drop pure identifiers and raw text date (we already have PurchaseTimestamp)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m drop_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPurchaseID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPurchaseDate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m df[TARGET_COL]\n\u001b[0;32m      6\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[TARGET_COL] \u001b[38;5;241m+\u001b[39m drop_cols)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "TARGET_COL = \"IsBadBuy\"  # change if your target name differs\n",
    "\n",
    "# Drop pure identifiers and raw text date (we already have PurchaseTimestamp)\n",
    "drop_cols = [c for c in [\"PurchaseID\", \"PurchaseDate\"] if c in df.columns]\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[TARGET_COL] + drop_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(\"Train/Test sizes:\", X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab23d02-bf50-469d-8a1b-b80632653e71",
   "metadata": {},
   "source": [
    "## 5) Preprocessing Pipeline (ColumnTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53195c37-3e96-4eae-b482-efe1dcc1928b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Identify feature types\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m numeric_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      6\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdifference(numeric_features)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Numeric: impute median then standardize\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from packaging import version\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.columns.difference(numeric_features).tolist()\n",
    "\n",
    "# Numeric: impute median then standardize\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())  # dense pipeline\n",
    "])\n",
    "\n",
    "# Categorical: impute mode then OHE (compatible with new sklearn)\n",
    "ohe_kwargs = dict(handle_unknown=\"ignore\")\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kwargs[\"sparse_output\"] = False   # new param name in newer sklearn\n",
    "else:\n",
    "    ohe_kwargs[\"sparse\"] = False          # legacy param name for older sklearn\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "print(\"Numeric features:\", len(numeric_features))\n",
    "print(\"Categorical features:\", len(categorical_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83720d00-0240-42df-83ff-90916a8182f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b93101-cae3-402a-b6ac-ad68da2a82c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

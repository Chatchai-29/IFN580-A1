{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90861ecd",
   "metadata": {},
   "source": [
    "\n",
    "# IFN580 – Assignment 1: Starter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8812c4",
   "metadata": {},
   "source": [
    "## 1) Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Config & Imports\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COL = \"IsBadBuy\"  # change if your target column differs\n",
    "!pip install imblearn\n",
    "\n",
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure inline plots in classic Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa679fd1",
   "metadata": {},
   "source": [
    "## 2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d25247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load Data (robust search + sentinel handling)\n",
    "DATA_PATHS = [\n",
    "    \"data/kick.csv\",\n",
    "    \"kick.csv\",\n",
    "    \"assignment 1 data kick.csv\",\n",
    "    \"./data/kick.csv\",\n",
    "]\n",
    "\n",
    "def find_data(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "DATA_PATH = find_data(DATA_PATHS)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"CSV not found. Put your dataset as 'data/kick.csv' or update DATA_PATHS.\")\n",
    "\n",
    "print(f\"Using data at: {DATA_PATH}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Turn '?' into NaN so imputers treat them as missing\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b90fed",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Quick Audit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcafe21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3) Quick Audit + numeric coercion (promote numeric-like object columns)\n",
    "\n",
    "# Convert object columns that look numeric into real numbers (heuristic)\n",
    "coerced = []\n",
    "for c in df.columns:\n",
    "    if c == TARGET_COL:  # don't touch the target\n",
    "        continue\n",
    "    s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    # treat as numeric if majority converts and at least some support\n",
    "    if (s.notna().mean() > 0.6) and (s.notna().sum() > 100):\n",
    "        df[c] = s\n",
    "        coerced.append(c)\n",
    "print(\"Coerced to numeric:\", len(coerced), \"| sample:\", coerced[:8])\n",
    "\n",
    "# Basic info for Task 1 evidence\n",
    "display(df.info())\n",
    "print(\"Target counts (before):\")\n",
    "display(df[TARGET_COL].value_counts(dropna=False))\n",
    "print(\"Target ratio (before):\")\n",
    "display(df[TARGET_COL].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648b0d6-7001-46c6-8bfb-19a334b6aea2",
   "metadata": {},
   "source": [
    "## 4)Train/Test Split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce7759-dfdf-48e8-982e-343729f1a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Train/Test Split (stratified) and drop non-predictive columns\n",
    "drop_cols = [c for c in [\"PurchaseID\", \"PurchaseDate\"] if c in df.columns]  # keep PurchaseTimestamp\n",
    "\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[TARGET_COL] + drop_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train/Test sizes:\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab23d02-bf50-469d-8a1b-b80632653e71",
   "metadata": {},
   "source": [
    "## 5) Preprocessing Pipeline (ColumnTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53195c37-3e96-4eae-b482-efe1dcc1928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Preprocessing Pipeline (ColumnTransformer) – sklearn-version safe\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.columns.difference(numeric_features).tolist()\n",
    "\n",
    "# Numeric: impute median then standardize (dense)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical: impute mode then One-Hot Encode\n",
    "# Guard for sklearn param rename: 'sparse' (old) -> 'sparse_output' (new)\n",
    "try:\n",
    "    _ = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", ohe)\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "print(\"Numeric features:\", len(numeric_features))\n",
    "print(\"Categorical features:\", len(categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c02b97-7660-4842-b2ce-d326b835ca4b",
   "metadata": {},
   "source": [
    "## task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122e698-8264-4766-962d-e54db116aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a\n",
    "\n",
    "# Instantiate the model\n",
    "dt_model = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Create a pipeline that first preprocesses the data and then trains the model\n",
    "dt_pipeline = Pipeline(steps=[('preprocessor', preprocess),\n",
    "                              ('classifier', dt_model)])\n",
    "\n",
    "# Train the pipeline\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Access the parameters of the trained Decision Tree model from the pipeline\n",
    "dt_params = dt_pipeline.named_steps['classifier'].get_params()\n",
    "\n",
    "# Display the parameters\n",
    "print(\"Decision Tree Model Parameters:\")\n",
    "for param, value in dt_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99375e5-c845-4c6b-bf13-b8dbca141e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1bParameters used for the train/test split\n",
    "test_size = 0.2\n",
    "random_state = RANDOM_STATE # From the initial config cell\n",
    "stratify = y # The target variable\n",
    "\n",
    "print(f\"Test set size: {test_size}\")\n",
    "print(f\"Random state: {random_state}\")\n",
    "print(f\"Stratification used: {stratify.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2749657-413c-457e-96bb-b7c4b9161691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1c\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the training data\n",
    "y_train_pred = dt_pipeline.predict(X_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = dt_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41403924-b787-4126-929a-55c2603258f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1e Get the trained Decision Tree model from the pipeline\n",
    "tree_model = dt_pipeline.named_steps['classifier']\n",
    "\n",
    "# Get the feature names after preprocessing\n",
    "preprocessor = dt_pipeline.named_steps['preprocessor']\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Get the feature index for the root node (first split)\n",
    "first_split_feature_index = tree_model.tree_.feature[0]\n",
    "first_split_feature_name = feature_names[first_split_feature_index]\n",
    "\n",
    "print(f\"The variable used for the first split is: {first_split_feature_name}\")\n",
    "print(\"\\nVariables used for splits at the second level:\")\n",
    "\n",
    "# Get the children of the root node (nodes at the second level)\n",
    "root_node_left_child = tree_model.tree_.children_left[0]\n",
    "root_node_right_child = tree_model.tree_.children_right[0]\n",
    "\n",
    "second_level_nodes = []\n",
    "if root_node_left_child != -1:\n",
    "    second_level_nodes.append(root_node_left_child)\n",
    "if root_node_right_child != -1:\n",
    "    second_level_nodes.append(root_node_right_child)\n",
    "\n",
    "\n",
    "for node_index in second_level_nodes:\n",
    "    # Check if the node is a split node (not a leaf node)\n",
    "    if tree_model.tree_.children_left[node_index] != -1 or tree_model.tree_.children_right[node_index] != -1:\n",
    "        feature_index = tree_model.tree_.feature[node_index]\n",
    "        feature_name = feature_names[feature_index]\n",
    "        print(f\"- Node {node_index}: {feature_name}\")\n",
    "    else:\n",
    "        print(f\"- Node {node_index}: (This is a leaf node, no split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a0df1-cbf3-459d-96ed-0400d50d3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1d Get the number of nodes\n",
    "n_nodes = tree_model.tree_.node_count\n",
    "\n",
    "# Get the number of rules (leaf nodes)\n",
    "# A node is a leaf node if its left and right children are both -1\n",
    "leaf_nodes = tree_model.tree_.children_left == tree_model.tree_.children_right\n",
    "n_rules = np.sum(leaf_nodes)\n",
    "\n",
    "print(f\"Number of nodes in the Decision Tree: {n_nodes}\")\n",
    "print(f\"Number of rules (leaf nodes) in the Decision Tree: {n_rules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f016fc-1d8f-4a9a-9826-a06c6931fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1fGet the feature importances\n",
    "importances = tree_model.feature_importances_\n",
    "\n",
    "# Create a pandas Series for easier sorting\n",
    "feature_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "# Get the top 5 most important features\n",
    "top_5_features = feature_importances.nlargest(5)\n",
    "\n",
    "print(\"Top 5 most important variables in building the tree:\")\n",
    "print(top_5_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0822d9f-9e8f-4b4b-9d12-b3669dbd1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1g\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "overfitting_threshold = 0.10  # You can adjust this threshold\n",
    "\n",
    "if (train_acc - test_acc) > overfitting_threshold:\n",
    "    print(\"\\nEvidence of potential overfitting: Training accuracy is significantly higher than test accuracy.\")\n",
    "else:\n",
    "    print(\"\\nNo strong evidence of overfitting based on the difference between training and test accuracy (within the set threshold).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9922a-0fff-4d4d-b7e6-b9093ea500bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2a\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 10, 20],\n",
    "    'classifier__min_samples_leaf': [1, 5, 10],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "# cv=5 means 5-fold cross-validation\n",
    "grid_search = GridSearchCV(dt_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "print(\"Performing Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid Search complete.\")\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"\\nOptimal parameters found by GridSearchCV:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best performing model\n",
    "best_dt_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29ad80-9188-470c-bda1-cf16724a3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the training data\n",
    "y_train_pred = grid_search.predict(X_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_acc_ = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_acc_:.4f}\")\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_acc_ = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_acc_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c95515-76ba-4d9c-9ac5-7134582d1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2c\n",
    "# Access the classifier and preprocessor inside the pipeline\n",
    "tree_model_   = best_dt_model.named_steps['classifier']\n",
    "preprocessor_ = best_dt_model.named_steps['preprocessor']\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = preprocessor_.get_feature_names_out()\n",
    "except Exception:\n",
    "    # fallback if get_feature_names_out not available\n",
    "    num_names = preprocessor.transformers_[0][2]  # numeric features\n",
    "    cat_enc   = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_names = list(cat_enc.get_feature_names_out(preprocessor.transformers_[1][2]))\n",
    "    feature_names = np.array(num_names + cat_names)\n",
    "\n",
    "# Tree stats\n",
    "n_nodes = tree_model_.tree_.node_count\n",
    "leaf_nodes = tree_model_.tree_.children_left == tree_model_.tree_.children_right\n",
    "n_rules = np.sum(leaf_nodes)\n",
    "\n",
    "print(f\"Number of nodes in the Decision Tree: {n_nodes}\")\n",
    "print(f\"Number of rules (leaf nodes) in the Decision Tree: {n_rules}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36e42a-023a-4cdc-87f2-3fa4daeb6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2d\n",
    "first_split_feature_index = tree_model_.tree_.feature[0]\n",
    "first_split_feature_name = feature_names[first_split_feature_index]\n",
    "\n",
    "print(f\"The variable used for the first split is: {first_split_feature_name}\")\n",
    "print(\"\\nVariables used for splits at the second level:\")\n",
    "\n",
    "# Get the children of the root node (nodes at the second level)\n",
    "root_node_left_child = tree_model_.tree_.children_left[0]\n",
    "root_node_right_child = tree_model_.tree_.children_right[0]\n",
    "\n",
    "second_level_nodes = []\n",
    "if root_node_left_child != -1:\n",
    "    second_level_nodes.append(root_node_left_child)\n",
    "if root_node_right_child != -1:\n",
    "    second_level_nodes.append(root_node_right_child)\n",
    "\n",
    "\n",
    "for node_index in second_level_nodes:\n",
    "    # Check if the node is a split node (not a leaf node)\n",
    "    if tree_model_.tree_.children_left[node_index] != -1 or tree_model_.tree_.children_right[node_index] != -1:\n",
    "        feature_index = tree_model_.tree_.feature[node_index]\n",
    "        feature_name = feature_names[feature_index]\n",
    "        print(f\"- Node {node_index}: {feature_name}\")\n",
    "    else:\n",
    "        print(f\"- Node {node_index}: (This is a leaf node, no split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7053ab-a9b2-4eb6-a19b-b2de4b1b837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2eGet the feature importances\n",
    "importances = tree_model_.feature_importances_\n",
    "\n",
    "# Create a pandas Series for easier sorting\n",
    "feature_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "# Get the top 5 most important features\n",
    "top_5_features = feature_importances.nlargest(5)\n",
    "\n",
    "print(\"Top 5 most important variables in building the tree:\")\n",
    "print(top_5_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70297a26-623e-4a5b-a1a8-2fc80127cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2f\n",
    "print(f\"Training Accuracy: {train_acc_:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_:.4f}\")\n",
    "overfitting_threshold = 0.10  # You can adjust this threshold\n",
    "\n",
    "if (train_acc_ - test_acc_) > overfitting_threshold:\n",
    "    print(\"\\nEvidence of potential overfitting: Training accuracy is significantly higher than test accuracy.\")\n",
    "else:\n",
    "    print(\"\\nNo strong evidence of overfitting based on the difference between training and test accuracy (within the set threshold).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032e6d1-83c9-4918-869d-81f2d206b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "# Probabilities for ROC curve\n",
    "y_test_proba_default = dt_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_test_proba_grid = best_dt_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curves and AUC\n",
    "fpr_default, tpr_default, _ = roc_curve(y_test, y_test_proba_default)\n",
    "auc_default = roc_auc_score(y_test, y_test_proba_default)\n",
    "\n",
    "fpr_grid, tpr_grid, _ = roc_curve(y_test, y_test_proba_grid)\n",
    "auc_grid = roc_auc_score(y_test, y_test_proba_grid)\n",
    "\n",
    "print(f\"Default DT AUC: {auc_default:.4f}\")\n",
    "print(f\"GridSearchCV DT AUC: {auc_grid:.4f}\")\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_default, tpr_default, label=f\"Default DT (AUC = {auc_default:.3f})\")\n",
    "plt.plot(fpr_grid, tpr_grid, label=f\"GridSearchCV DT (AUC = {auc_grid:.3f})\")\n",
    "plt.plot([0,1], [0,1], 'k--')  # baseline\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison: Default vs GridSearchCV Decision Tree\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11514239-d2ed-4898-b7a4-686e2690f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "# Get the classifier and preprocessor from the tuned pipeline\n",
    "tree_model_best = best_dt_model.named_steps['classifier']\n",
    "preprocessor_best = best_dt_model.named_steps['preprocessor']\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = preprocessor_best.get_feature_names_out()\n",
    "except Exception:\n",
    "    num_names = preprocessor_best.transformers_[0][2]\n",
    "    cat_enc   = preprocessor_best.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_names = list(cat_enc.get_feature_names_out(preprocessor_best.transformers_[1][2]))\n",
    "    feature_names = np.array(num_names + cat_names)\n",
    "\n",
    "# Get top 10 most important features\n",
    "importances = tree_model_best.feature_importances_\n",
    "feature_importances = pd.Series(importances, index=feature_names)\n",
    "top_features = feature_importances.nlargest(10)\n",
    "\n",
    "print(\"Top features that contribute to predicting 'kicks':\")\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b1068-6b8f-4128-a6be-6720ce47764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse from Task 1: preprocess, X_train, X_test, y_train, y_test\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Fixed `RANDOM_STATE` for repeatable results.\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a1d7d-8ec2-4b92-a364-a724cdeca625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing Task-1 preprocessing in a single end-to-end pipeline\n",
    "pipe_full = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning: regularisation type & strength\n",
    "# L1/L2 regularisations control sparsity vs ridge-style shrinkage\n",
    "# Liblinear/saga support L1\n",
    "param_grid_full = {\n",
    "    \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "    \"clf__C\": [0.01, 0.1, 1, 10],\n",
    "    \"clf__solver\": [\"liblinear\", \"saga\"],\n",
    "}\n",
    "\n",
    "# 5-fold CV tuned for accuracy\n",
    "grid_full = GridSearchCV(pipe_full, param_grid_full, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_full.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "ytr_pred_full = grid_full.predict(X_train)\n",
    "yte_pred_full = grid_full.predict(X_test)\n",
    "\n",
    "print(\"Best params:\", grid_full.best_params_)\n",
    "print(\"Train acc:\", accuracy_score(y_train, ytr_pred_full))\n",
    "print(\"Test  acc:\", accuracy_score(y_test, yte_pred_full))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, yte_pred_full))\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, yte_pred_full))\n",
    "\n",
    "# Top 5 features\n",
    "best_full = grid_full.best_estimator_\n",
    "feat_names = best_full.named_steps[\"prep\"].get_feature_names_out()\n",
    "coefs = best_full.named_steps[\"clf\"].coef_[0]\n",
    "top5 = pd.DataFrame({\"Feature\": feat_names, \"Coef\": coefs, \"Abs\": np.abs(coefs)}).sort_values(\"Abs\", ascending=False).head(5)\n",
    "display(top5)\n",
    "\n",
    "# Using predicted probabilities to compute ROC/AUC\n",
    "yprob_full = best_full.predict_proba(X_test)[:,1]\n",
    "fpr_full, tpr_full, _ = roc_curve(y_test, yprob_full)\n",
    "auc_full = auc(fpr_full, tpr_full)\n",
    "plt.plot(fpr_full, tpr_full, label=f\"Full (AUC={auc_full:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e989af-038d-42df-8d5a-0f4577868eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base estimator is used by RFE to score feature usefulness\n",
    "base_lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, solver=\"liblinear\", penalty=\"l2\")\n",
    "\n",
    "pipe_rfe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"rfe\", RFE(base_lr, n_features_to_select=50, step=0.1)),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Hyperparameters to tune:\n",
    "# How many features RFE should keep\n",
    "# Logistic penalty type & regularisation strength (C)\n",
    "# Compatible solvers\n",
    "param_grid_rfe = {\n",
    "    \"rfe__n_features_to_select\": [30, 50, 80],\n",
    "    \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "    \"clf__C\": [0.01, 0.1, 1, 10],\n",
    "    \"clf__solver\": [\"liblinear\", \"saga\"],\n",
    "}\n",
    "\n",
    "# 5-fold CV tuned for accuracy\n",
    "grid_rfe = GridSearchCV(pipe_rfe, param_grid_rfe, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_rfe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "ytr_pred_rfe = grid_rfe.predict(X_train)\n",
    "yte_pred_rfe = grid_rfe.predict(X_test)\n",
    "\n",
    "print(\"Best params (RFE):\", grid_rfe.best_params_)\n",
    "print(\"Train acc:\", accuracy_score(y_train, ytr_pred_rfe))\n",
    "print(\"Test  acc:\", accuracy_score(y_test, yte_pred_rfe))\n",
    "\n",
    "# Plotting the ROC curve\n",
    "best_rfe = grid_rfe.best_estimator_\n",
    "yprob_rfe = best_rfe.predict_proba(X_test)[:,1]\n",
    "fpr_rfe, tpr_rfe, _ = roc_curve(y_test, yprob_rfe)\n",
    "auc_rfe = auc(fpr_rfe, tpr_rfe)\n",
    "plt.plot(fpr_rfe, tpr_rfe, label=f\"RFE (AUC={auc_rfe:.3f})\")\n",
    "\n",
    "# Plotting the diagonal reference and finalising the ROC figure\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.legend(); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curves Logistic Regression\"); plt.show()\n",
    "\n",
    "# Checking overfitting (comparing train vs test accuracy gaps)\n",
    "gap_full = accuracy_score(y_train, ytr_pred_full) - accuracy_score(y_test, yte_pred_full)\n",
    "gap_rfe  = accuracy_score(y_train, ytr_pred_rfe) - accuracy_score(y_test, yte_pred_rfe)\n",
    "print(\"Overfitting gap (Full):\", gap_full)\n",
    "print(\"Overfitting gap (RFE):\", gap_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ed79a-6ff6-45fc-88f6-7e7d0479e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the better model by test accuracy\n",
    "# If RFE model tests are atleast as good as the Full model, we will prefer RFE for simplicity\n",
    "use_rfe = accuracy_score(y_test, yte_pred_rfe) >= accuracy_score(y_test, yte_pred_full)\n",
    "chosen = best_rfe if use_rfe else best_full\n",
    "model_name = \"RFE\" if use_rfe else \"Full\"\n",
    "\n",
    "# Getting the post preprocessing feature names (after OHE)\n",
    "feat_names = chosen.named_steps[\"prep\"].get_feature_names_out()\n",
    "coefs = chosen.named_steps[\"clf\"].coef_[0]\n",
    "\n",
    "# Ranking by absolute value\n",
    "# Larger coef means stronger effect on log-odds of IsBadBuy=1\n",
    "top10 = pd.DataFrame({\"Feature\": feat_names, \"Coef\": coefs, \"Abs\": np.abs(coefs)}).sort_values(\"Abs\", ascending=False).head(10)\n",
    "print(\"Top contributors from\", model_name)\n",
    "display(top10)\n",
    "\n",
    "# Positive coef means higher odds of IsBadBuy=1, negative coef means lower odds\n",
    "print(\"Positive coef -> higher odds of IsBadBuy=1 (likely kick); Negative -> lower odds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656150c5-6021-4ae1-ae49-7cfef8a8c70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

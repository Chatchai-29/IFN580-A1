{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90861ecd",
   "metadata": {},
   "source": [
    "\n",
    "# IFN580 – Assignment 1: Starter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd565c5-35d0-4c27-ae83-dbda10ac8b49",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1) Config & Imports\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COL = \"IsBadBuy\"  # change if your target column differs\n",
    "!pip install imblearn\n",
    "\n",
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure inline plots in classic Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d25247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2) Load Data (robust search + sentinel handling)\n",
    "DATA_PATHS = [\n",
    "    \"data/kick.csv\",\n",
    "    \"kick.csv\",\n",
    "    \"assignment 1 data kick.csv\",\n",
    "    \"./data/kick.csv\",\n",
    "]\n",
    "\n",
    "def find_data(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "DATA_PATH = find_data(DATA_PATHS)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"CSV not found. Put your dataset as 'data/kick.csv' or update DATA_PATHS.\")\n",
    "\n",
    "print(f\"Using data at: {DATA_PATH}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Turn '?' into NaN so imputers treat them as missing\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcafe21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.3) Quick Audit + numeric coercion (promote numeric-like object columns)\n",
    "\n",
    "# Convert object columns that look numeric into real numbers (heuristic)\n",
    "coerced = []\n",
    "for c in df.columns:\n",
    "    if c == TARGET_COL:  # don't touch the target\n",
    "        continue\n",
    "    s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    # treat as numeric if majority converts and at least some support\n",
    "    if (s.notna().mean() > 0.6) and (s.notna().sum() > 100):\n",
    "        df[c] = s\n",
    "        coerced.append(c)\n",
    "print(\"Coerced to numeric:\", len(coerced), \"| sample:\", coerced[:8])\n",
    "\n",
    "# Basic info for Task 1 evidence\n",
    "display(df.info())\n",
    "print(\"Target counts (before):\")\n",
    "display(df[TARGET_COL].value_counts(dropna=False))\n",
    "print(\"Target ratio (before):\")\n",
    "display(df[TARGET_COL].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce7759-dfdf-48e8-982e-343729f1a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4) Train/Test Split (stratified) and drop non-predictive columns\n",
    "drop_cols = [c for c in [\"PurchaseID\", \"PurchaseDate\"] if c in df.columns]  # keep PurchaseTimestamp\n",
    "\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[TARGET_COL] + drop_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train/Test sizes:\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53195c37-3e96-4eae-b482-efe1dcc1928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5) Preprocessing Pipeline (ColumnTransformer) – sklearn-version safe\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.columns.difference(numeric_features).tolist()\n",
    "\n",
    "# Numeric: impute median then standardize (dense)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical: impute mode then One-Hot Encode\n",
    "# Guard for sklearn param rename: 'sparse' (old) -> 'sparse_output' (new)\n",
    "try:\n",
    "    _ = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", ohe)\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "print(\"Numeric features:\", len(numeric_features))\n",
    "print(\"Categorical features:\", len(categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c02b97-7660-4842-b2ce-d326b835ca4b",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122e698-8264-4766-962d-e54db116aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a\n",
    "\n",
    "# Instantiate the model\n",
    "dt_model = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Create a pipeline that first preprocesses the data and then trains the model\n",
    "dt_pipeline = Pipeline(steps=[('preprocessor', preprocess),\n",
    "                              ('classifier', dt_model)])\n",
    "\n",
    "# Train the pipeline\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Access the parameters of the trained Decision Tree model from the pipeline\n",
    "dt_params = dt_pipeline.named_steps['classifier'].get_params()\n",
    "\n",
    "# Display the parameters\n",
    "print(\"Decision Tree Model Parameters:\")\n",
    "for param, value in dt_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99375e5-c845-4c6b-bf13-b8dbca141e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1bParameters used for the train/test split\n",
    "test_size = 0.2\n",
    "random_state = RANDOM_STATE # From the initial config cell\n",
    "stratify = y # The target variable\n",
    "\n",
    "print(f\"Test set size: {test_size}\")\n",
    "print(f\"Random state: {random_state}\")\n",
    "print(f\"Stratification used: {stratify.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2749657-413c-457e-96bb-b7c4b9161691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1c\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the training data\n",
    "y_train_pred = dt_pipeline.predict(X_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = dt_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41403924-b787-4126-929a-55c2603258f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1e Get the trained Decision Tree model from the pipeline\n",
    "tree_model = dt_pipeline.named_steps['classifier']\n",
    "\n",
    "# Get the feature names after preprocessing\n",
    "preprocessor = dt_pipeline.named_steps['preprocessor']\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Get the feature index for the root node (first split)\n",
    "first_split_feature_index = tree_model.tree_.feature[0]\n",
    "first_split_feature_name = feature_names[first_split_feature_index]\n",
    "\n",
    "print(f\"The variable used for the first split is: {first_split_feature_name}\")\n",
    "print(\"\\nVariables used for splits at the second level:\")\n",
    "\n",
    "# Get the children of the root node (nodes at the second level)\n",
    "root_node_left_child = tree_model.tree_.children_left[0]\n",
    "root_node_right_child = tree_model.tree_.children_right[0]\n",
    "\n",
    "second_level_nodes = []\n",
    "if root_node_left_child != -1:\n",
    "    second_level_nodes.append(root_node_left_child)\n",
    "if root_node_right_child != -1:\n",
    "    second_level_nodes.append(root_node_right_child)\n",
    "\n",
    "\n",
    "for node_index in second_level_nodes:\n",
    "    # Check if the node is a split node (not a leaf node)\n",
    "    if tree_model.tree_.children_left[node_index] != -1 or tree_model.tree_.children_right[node_index] != -1:\n",
    "        feature_index = tree_model.tree_.feature[node_index]\n",
    "        feature_name = feature_names[feature_index]\n",
    "        print(f\"- Node {node_index}: {feature_name}\")\n",
    "    else:\n",
    "        print(f\"- Node {node_index}: (This is a leaf node, no split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a0df1-cbf3-459d-96ed-0400d50d3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1d Get the number of nodes\n",
    "n_nodes = tree_model.tree_.node_count\n",
    "\n",
    "# Get the number of rules (leaf nodes)\n",
    "# A node is a leaf node if its left and right children are both -1\n",
    "leaf_nodes = tree_model.tree_.children_left == tree_model.tree_.children_right\n",
    "n_rules = np.sum(leaf_nodes)\n",
    "\n",
    "print(f\"Number of nodes in the Decision Tree: {n_nodes}\")\n",
    "print(f\"Number of rules (leaf nodes) in the Decision Tree: {n_rules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f016fc-1d8f-4a9a-9826-a06c6931fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1fGet the feature importances\n",
    "importances = tree_model.feature_importances_\n",
    "\n",
    "# Create a pandas Series for easier sorting\n",
    "feature_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "# Get the top 5 most important features\n",
    "top_5_features = feature_importances.nlargest(5)\n",
    "\n",
    "print(\"Top 5 most important variables in building the tree:\")\n",
    "print(top_5_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0822d9f-9e8f-4b4b-9d12-b3669dbd1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1g\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "overfitting_threshold = 0.10  # You can adjust this threshold\n",
    "\n",
    "if (train_acc - test_acc) > overfitting_threshold:\n",
    "    print(\"\\nEvidence of potential overfitting: Training accuracy is significantly higher than test accuracy.\")\n",
    "else:\n",
    "    print(\"\\nNo strong evidence of overfitting based on the difference between training and test accuracy (within the set threshold).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9922a-0fff-4d4d-b7e6-b9093ea500bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2a\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 10, 20],\n",
    "    'classifier__min_samples_leaf': [1, 5, 10],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "# cv=5 means 5-fold cross-validation\n",
    "grid_search = GridSearchCV(dt_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "print(\"Performing Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid Search complete.\")\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"\\nOptimal parameters found by GridSearchCV:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best performing model\n",
    "best_dt_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29ad80-9188-470c-bda1-cf16724a3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the training data\n",
    "y_train_pred = grid_search.predict(X_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_acc_ = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_acc_:.4f}\")\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_acc_ = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_acc_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c95515-76ba-4d9c-9ac5-7134582d1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2c\n",
    "# Access the classifier and preprocessor inside the pipeline\n",
    "tree_model_   = best_dt_model.named_steps['classifier']\n",
    "preprocessor_ = best_dt_model.named_steps['preprocessor']\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = preprocessor_.get_feature_names_out()\n",
    "except Exception:\n",
    "    # fallback if get_feature_names_out not available\n",
    "    num_names = preprocessor.transformers_[0][2]  # numeric features\n",
    "    cat_enc   = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_names = list(cat_enc.get_feature_names_out(preprocessor.transformers_[1][2]))\n",
    "    feature_names = np.array(num_names + cat_names)\n",
    "\n",
    "# Tree stats\n",
    "n_nodes = tree_model_.tree_.node_count\n",
    "leaf_nodes = tree_model_.tree_.children_left == tree_model_.tree_.children_right\n",
    "n_rules = np.sum(leaf_nodes)\n",
    "\n",
    "print(f\"Number of nodes in the Decision Tree: {n_nodes}\")\n",
    "print(f\"Number of rules (leaf nodes) in the Decision Tree: {n_rules}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36e42a-023a-4cdc-87f2-3fa4daeb6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2d\n",
    "first_split_feature_index = tree_model_.tree_.feature[0]\n",
    "first_split_feature_name = feature_names[first_split_feature_index]\n",
    "\n",
    "print(f\"The variable used for the first split is: {first_split_feature_name}\")\n",
    "print(\"\\nVariables used for splits at the second level:\")\n",
    "\n",
    "# Get the children of the root node (nodes at the second level)\n",
    "root_node_left_child = tree_model_.tree_.children_left[0]\n",
    "root_node_right_child = tree_model_.tree_.children_right[0]\n",
    "\n",
    "second_level_nodes = []\n",
    "if root_node_left_child != -1:\n",
    "    second_level_nodes.append(root_node_left_child)\n",
    "if root_node_right_child != -1:\n",
    "    second_level_nodes.append(root_node_right_child)\n",
    "\n",
    "\n",
    "for node_index in second_level_nodes:\n",
    "    # Check if the node is a split node (not a leaf node)\n",
    "    if tree_model_.tree_.children_left[node_index] != -1 or tree_model_.tree_.children_right[node_index] != -1:\n",
    "        feature_index = tree_model_.tree_.feature[node_index]\n",
    "        feature_name = feature_names[feature_index]\n",
    "        print(f\"- Node {node_index}: {feature_name}\")\n",
    "    else:\n",
    "        print(f\"- Node {node_index}: (This is a leaf node, no split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7053ab-a9b2-4eb6-a19b-b2de4b1b837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2eGet the feature importances\n",
    "importances = tree_model_.feature_importances_\n",
    "\n",
    "# Create a pandas Series for easier sorting\n",
    "feature_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "# Get the top 5 most important features\n",
    "top_5_features = feature_importances.nlargest(5)\n",
    "\n",
    "print(\"Top 5 most important variables in building the tree:\")\n",
    "print(top_5_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70297a26-623e-4a5b-a1a8-2fc80127cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2f\n",
    "print(f\"Training Accuracy: {train_acc_:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_:.4f}\")\n",
    "overfitting_threshold = 0.10  # You can adjust this threshold\n",
    "\n",
    "if (train_acc_ - test_acc_) > overfitting_threshold:\n",
    "    print(\"\\nEvidence of potential overfitting: Training accuracy is significantly higher than test accuracy.\")\n",
    "else:\n",
    "    print(\"\\nNo strong evidence of overfitting based on the difference between training and test accuracy (within the set threshold).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032e6d1-83c9-4918-869d-81f2d206b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "# Probabilities for ROC curve\n",
    "y_test_proba_default = dt_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_test_proba_grid = best_dt_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curves and AUC\n",
    "fpr_default, tpr_default, _ = roc_curve(y_test, y_test_proba_default)\n",
    "auc_default = roc_auc_score(y_test, y_test_proba_default)\n",
    "\n",
    "fpr_grid, tpr_grid, _ = roc_curve(y_test, y_test_proba_grid)\n",
    "auc_grid = roc_auc_score(y_test, y_test_proba_grid)\n",
    "\n",
    "print(f\"Default DT AUC: {auc_default:.4f}\")\n",
    "print(f\"GridSearchCV DT AUC: {auc_grid:.4f}\")\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_default, tpr_default, label=f\"Default DT (AUC = {auc_default:.3f})\")\n",
    "plt.plot(fpr_grid, tpr_grid, label=f\"GridSearchCV DT (AUC = {auc_grid:.3f})\")\n",
    "plt.plot([0,1], [0,1], 'k--')  # baseline\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison: Default vs GridSearchCV Decision Tree\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11514239-d2ed-4898-b7a4-686e2690f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "# Get the classifier and preprocessor from the tuned pipeline\n",
    "tree_model_best = best_dt_model.named_steps['classifier']\n",
    "preprocessor_best = best_dt_model.named_steps['preprocessor']\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = preprocessor_best.get_feature_names_out()\n",
    "except Exception:\n",
    "    num_names = preprocessor_best.transformers_[0][2]\n",
    "    cat_enc   = preprocessor_best.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_names = list(cat_enc.get_feature_names_out(preprocessor_best.transformers_[1][2]))\n",
    "    feature_names = np.array(num_names + cat_names)\n",
    "\n",
    "# Get top 10 most important features\n",
    "importances = tree_model_best.feature_importances_\n",
    "feature_importances = pd.Series(importances, index=feature_names)\n",
    "top_features = feature_importances.nlargest(10)\n",
    "\n",
    "print(\"Top features that contribute to predicting 'kicks':\")\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa172e6-0b3f-4866-aaba-9d3360b4d853",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b1068-6b8f-4128-a6be-6720ce47764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse from Task 1: preprocess, X_train, X_test, y_train, y_test\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Fixed `RANDOM_STATE` for repeatable results.\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a1d7d-8ec2-4b92-a364-a724cdeca625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing Task-1 preprocessing in a single end-to-end pipeline\n",
    "pipe_full = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning: regularisation type & strength\n",
    "# L1/L2 regularisations control sparsity vs ridge-style shrinkage\n",
    "# Liblinear/saga support L1\n",
    "param_grid_full = {\n",
    "    \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "    \"clf__C\": [0.01, 0.1, 1, 10],\n",
    "    \"clf__solver\": [\"liblinear\", \"saga\"],\n",
    "}\n",
    "\n",
    "# 5-fold CV tuned for accuracy\n",
    "grid_full = GridSearchCV(pipe_full, param_grid_full, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_full.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "ytr_pred_full = grid_full.predict(X_train)\n",
    "yte_pred_full = grid_full.predict(X_test)\n",
    "\n",
    "print(\"Best params:\", grid_full.best_params_)\n",
    "print(\"Train acc:\", accuracy_score(y_train, ytr_pred_full))\n",
    "print(\"Test  acc:\", accuracy_score(y_test, yte_pred_full))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, yte_pred_full))\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, yte_pred_full))\n",
    "\n",
    "# Top 5 features\n",
    "best_full = grid_full.best_estimator_\n",
    "feat_names = best_full.named_steps[\"prep\"].get_feature_names_out()\n",
    "coefs = best_full.named_steps[\"clf\"].coef_[0]\n",
    "top5 = pd.DataFrame({\"Feature\": feat_names, \"Coef\": coefs, \"Abs\": np.abs(coefs)}).sort_values(\"Abs\", ascending=False).head(5)\n",
    "display(top5)\n",
    "\n",
    "# Using predicted probabilities to compute ROC/AUC\n",
    "yprob_full = best_full.predict_proba(X_test)[:,1]\n",
    "fpr_full, tpr_full, _ = roc_curve(y_test, yprob_full)\n",
    "auc_full = auc(fpr_full, tpr_full)\n",
    "plt.plot(fpr_full, tpr_full, label=f\"Full (AUC={auc_full:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e989af-038d-42df-8d5a-0f4577868eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base estimator is used by RFE to score feature usefulness\n",
    "base_lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, solver=\"liblinear\", penalty=\"l2\")\n",
    "\n",
    "pipe_rfe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"rfe\", RFE(base_lr, n_features_to_select=50, step=0.1)),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Hyperparameters to tune:\n",
    "# How many features RFE should keep\n",
    "# Logistic penalty type & regularisation strength (C)\n",
    "# Compatible solvers\n",
    "param_grid_rfe = {\n",
    "    \"rfe__n_features_to_select\": [30, 50, 80],\n",
    "    \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "    \"clf__C\": [0.01, 0.1, 1, 10],\n",
    "    \"clf__solver\": [\"liblinear\", \"saga\"],\n",
    "}\n",
    "\n",
    "# 5-fold CV tuned for accuracy\n",
    "grid_rfe = GridSearchCV(pipe_rfe, param_grid_rfe, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_rfe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "ytr_pred_rfe = grid_rfe.predict(X_train)\n",
    "yte_pred_rfe = grid_rfe.predict(X_test)\n",
    "\n",
    "print(\"Best params (RFE):\", grid_rfe.best_params_)\n",
    "print(\"Train acc:\", accuracy_score(y_train, ytr_pred_rfe))\n",
    "print(\"Test  acc:\", accuracy_score(y_test, yte_pred_rfe))\n",
    "\n",
    "# Plotting the ROC curve\n",
    "best_rfe = grid_rfe.best_estimator_\n",
    "yprob_rfe = best_rfe.predict_proba(X_test)[:,1]\n",
    "fpr_rfe, tpr_rfe, _ = roc_curve(y_test, yprob_rfe)\n",
    "auc_rfe = auc(fpr_rfe, tpr_rfe)\n",
    "plt.plot(fpr_rfe, tpr_rfe, label=f\"RFE (AUC={auc_rfe:.3f})\")\n",
    "\n",
    "# Plotting the diagonal reference and finalising the ROC figure\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.legend(); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curves Logistic Regression\"); plt.show()\n",
    "\n",
    "# Checking overfitting (comparing train vs test accuracy gaps)\n",
    "gap_full = accuracy_score(y_train, ytr_pred_full) - accuracy_score(y_test, yte_pred_full)\n",
    "gap_rfe  = accuracy_score(y_train, ytr_pred_rfe) - accuracy_score(y_test, yte_pred_rfe)\n",
    "print(\"Overfitting gap (Full):\", gap_full)\n",
    "print(\"Overfitting gap (RFE):\", gap_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656150c5-6021-4ae1-ae49-7cfef8a8c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the better model by test accuracy\n",
    "# If RFE model tests are atleast as good as the Full model, we will prefer RFE for simplicity\n",
    "use_rfe = accuracy_score(y_test, yte_pred_rfe) >= accuracy_score(y_test, yte_pred_full)\n",
    "chosen = best_rfe if use_rfe else best_full\n",
    "model_name = \"RFE\" if use_rfe else \"Full\"\n",
    "\n",
    "# Getting the post processing feature names, if RFE is better we keep a subset\n",
    "feat_names_all = chosen.named_steps[\"prep\"].get_feature_names_out()\n",
    "if \"rfe\" in chosen.named_steps:\n",
    "    mask = chosen.named_steps[\"rfe\"].support_\n",
    "    feat_names = feat_names_all[mask]\n",
    "else:\n",
    "    feat_names = feat_names_all\n",
    "\n",
    "# Coefficients from the final LR\n",
    "coefs = chosen.named_steps[\"clf\"].coef_.ravel()\n",
    "print(f\"{model_name} features: {len(feat_names)}, coefficients: {len(coefs)}\")\n",
    "\n",
    "coef_df = (pd.DataFrame({\"Feature\": feat_names, \"Coef\": coefs})\n",
    "             .assign(Abs=lambda d: d[\"Coef\"].abs())\n",
    "             .sort_values(\"Abs\", ascending=False))\n",
    "top10 = coef_df.head(10)\n",
    "\n",
    "print(\"Top contributors from\", model_name)\n",
    "display(top10)\n",
    "\n",
    "# Positive coef means higher odds of IsBadBuy=1, negative coef means lower odds\n",
    "print(\"Positive coef -> higher odds of IsBadBuy=1 (likely kick); Negative -> lower odds.\")\n",
    "print(\"Note: one-hot effects are relative to the baseline category.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d478007-8df9-44a1-9b09-ed96e6958b98",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae9454-902d-4ba6-9a2f-74f100360970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 4 BOOTSTRAP (robust & simple) ===\n",
    "# Fix mixed-type categorical issues and avoid running OneHot inside the DT grid.\n",
    "\n",
    "import os, glob, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Defaults\n",
    "if \"RANDOM_STATE\" not in globals(): RANDOM_STATE = 42\n",
    "if \"TARGET_COL\"   not in globals(): TARGET_COL   = \"IsBadBuy\"\n",
    "\n",
    "# 1) Load df if missing\n",
    "def _find_csv():\n",
    "    for p in [\"data/kick.csv\", \"kick.csv\", \"assignment 1 data kick.csv\"]:\n",
    "        if os.path.isfile(p): return p\n",
    "    for p in glob.glob(\"**/*.csv\", recursive=True):\n",
    "        if \"kick\" in os.path.basename(p).lower(): return p\n",
    "    return None\n",
    "\n",
    "if \"df\" not in globals():\n",
    "    path = _find_csv()\n",
    "    if not path:\n",
    "        raise FileNotFoundError(\"kick.csv not found. Put it in ./data/ or next to the notebook.\")\n",
    "    # Avoid dtype fragmentation warnings\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    # Turn '?' into NaN\n",
    "    df.replace(\"?\", np.nan, inplace=True)\n",
    "    # Promote numeric-like object columns to numeric (simple heuristic)\n",
    "    for c in df.columns:\n",
    "        if c == TARGET_COL:\n",
    "            continue\n",
    "        if df[c].dtype == \"object\":\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if s.notna().mean() > 0.6:  # majority numeric -> treat as numeric\n",
    "                df[c] = s\n",
    "\n",
    "# 2) Train/test split if missing\n",
    "if not all(v in globals() for v in [\"X_train\",\"X_test\",\"y_train\",\"y_test\"]):\n",
    "    drop_cols = [c for c in [\"PurchaseID\",\"PurchaseDate\"] if c in df.columns]\n",
    "    y = df[TARGET_COL]\n",
    "    X = df.drop(columns=[TARGET_COL] + drop_cols)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "# 3) Preprocess pipeline (categorical forced to string AFTER impute)\n",
    "if \"preprocess\" not in globals():\n",
    "    num_feat = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_feat = X_train.columns.difference(num_feat).tolist()\n",
    "\n",
    "    numeric = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    categorical = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"to_str\", FunctionTransformer(lambda X: X.astype(object).astype(str))),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "    preprocess = ColumnTransformer(\n",
    "        [(\"num\", numeric, num_feat),\n",
    "         (\"cat\", categorical, cat_feat)],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "# 4) Fit preprocess ONCE and grid-search DT on the preprocessed matrix (no OneHot inside the grid)\n",
    "if \"dt_best\" not in globals():\n",
    "    # Fit preprocessor on train and transform to numeric matrix\n",
    "    prep_fitted = preprocess.fit(X_train, y_train)\n",
    "    Xtr = prep_fitted.transform(X_train)\n",
    "    # Ensure dense\n",
    "    if hasattr(Xtr, \"toarray\"): Xtr = Xtr.toarray()\n",
    "\n",
    "    # Now search a Decision Tree directly on Xtr (pure numeric) -> avoids mixed-type encoder issues\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "    grid = {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [3, 5, 7, None],\n",
    "        \"min_samples_split\": [2, 10, 50],\n",
    "        \"min_samples_leaf\": [1, 5, 25],\n",
    "    }\n",
    "    dt_search = GridSearchCV(dt, grid, scoring=\"roc_auc\", cv=cv, n_jobs=-1, refit=True)\n",
    "    dt_search.fit(Xtr, y_train)\n",
    "    best_dt = dt_search.best_estimator_\n",
    "\n",
    "    # Rebuild a Pipeline so that downstream code can use dt_best.named_steps[\"prep\"]/[\"clf\"]\n",
    "    dt_best = Pipeline([(\"prep\", prep_fitted), (\"clf\", best_dt)])\n",
    "    print(\"[Task4 bootstrap] Fitted preprocess and tuned DecisionTree on preprocessed matrix.\")\n",
    "else:\n",
    "    print(\"[Task4 bootstrap] Using existing dt_best / preprocess / split.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ff4ec-7254-4e45-b42f-ef9ad3c41369",
   "metadata": {},
   "source": [
    "**Task 4.1 — Additional processing for NN**\n",
    "\n",
    "- Numeric features are **standardized** (zero-mean / unit-variance) via `StandardScaler`.\n",
    "- Categorical features are **one-hot encoded** with `OneHotEncoder(handle_unknown=\"ignore\")`.\n",
    "- Missing values are handled by `SimpleImputer` (median for numeric, most_frequent for categorical).\n",
    "- These steps are already defined in our `preprocess` from Task 1 and applied *consistently* to train/test to avoid leakage.\n",
    "- Standardization is **important for NN** to stabilize gradients and speed up convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573cb89-c6c8-406c-ab7e-3b2a31f9a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.2 — NN with full feature set\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "# --- A) Build preprocessed matrices (use the already-fitted DT pipeline's preprocessor if available) ---\n",
    "# This keeps one-hot columns consistent with Task 2.\n",
    "if 'dt_best' in globals():\n",
    "    prep_for_nn = dt_best.named_steps['prep']\n",
    "else:\n",
    "    # fallback: fit our preprocess on training data (still simple)\n",
    "    prep_for_nn = preprocess.fit(X_train, y_train)\n",
    "\n",
    "Xtr = prep_for_nn.transform(X_train)\n",
    "Xte = prep_for_nn.transform(X_test)\n",
    "\n",
    "# Safety: ensure dense arrays for scikit-learn MLP\n",
    "if hasattr(Xtr, \"toarray\"): Xtr = Xtr.toarray()\n",
    "if hasattr(Xte, \"toarray\"): Xte = Xte.toarray()\n",
    "\n",
    "# --- helper functions ---\n",
    "def nn_scores(model, name=\"\"):\n",
    "    yhat_tr = model.predict(Xtr)\n",
    "    yhat_te = model.predict(Xte)\n",
    "    acc_tr  = accuracy_score(y_train, yhat_tr)\n",
    "    acc_te  = accuracy_score(y_test,  yhat_te)\n",
    "    proba_te = model.predict_proba(Xte)[:,1] if hasattr(model, \"predict_proba\") else model.decision_function(Xte)\n",
    "    auc_te = roc_auc_score(y_test, proba_te)\n",
    "    print(f\"{name} | Train acc={acc_tr:.3f}  Test acc={acc_te:.3f}  Test AUC={auc_te:.3f}\")\n",
    "    return acc_tr, acc_te, auc_te\n",
    "\n",
    "def plot_roc_simple(model, title):\n",
    "    proba_te = model.predict_proba(Xte)[:,1] if hasattr(model, \"predict_proba\") else model.decision_function(Xte)\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba_te)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"AUC={roc_auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1],'k--',lw=1)\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(title)\n",
    "    plt.legend(loc=\"lower right\"); plt.show()\n",
    "    return roc_auc\n",
    "\n",
    "# --- B) Baseline NN ---\n",
    "nn_base = MLPClassifier(\n",
    "    hidden_layer_sizes=(64,), activation=\"relu\", solver=\"adam\",\n",
    "    max_iter=300, early_stopping=True, random_state=RANDOM_STATE\n",
    ")\n",
    "nn_base.fit(Xtr, y_train)\n",
    "_ = nn_scores(nn_base, \"NN baseline (full)\")\n",
    "_ = plot_roc_simple(nn_base, \"ROC — NN baseline (full)\")\n",
    "\n",
    "# --- C) Tuned NN via simple GridSearch ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "nn_gs = GridSearchCV(\n",
    "    estimator=MLPClassifier(random_state=RANDOM_STATE, early_stopping=True, solver=\"adam\"),\n",
    "    param_grid={\n",
    "        \"hidden_layer_sizes\": [(64,), (128,), (64,32)],\n",
    "        \"activation\": [\"relu\", \"tanh\"],\n",
    "        \"alpha\": [1e-4, 1e-3],                # L2\n",
    "        \"learning_rate_init\": [1e-3, 3e-3],   # step size\n",
    "        \"batch_size\": [128, 256],\n",
    "        \"max_iter\": [300]\n",
    "    },\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit=True\n",
    ")\n",
    "nn_gs.fit(Xtr, y_train)\n",
    "nn_full = nn_gs.best_estimator_\n",
    "\n",
    "print(\"Best params (NN full):\", nn_gs.best_params_)\n",
    "acc_tr_full, acc_te_full, auc_te_full = nn_scores(nn_full, \"NN tuned (full)\")\n",
    "auc_full_curve = plot_roc_simple(nn_full, \"ROC — NN tuned (full)\")\n",
    "\n",
    "# Convergence / overfitting notes for the report\n",
    "print(f\"Converged iters: {nn_full.n_iter_}  (max_iter={nn_full.max_iter}, early_stopping={nn_full.early_stopping})\")\n",
    "print(f\"Overfitting gap (train - test acc): {acc_tr_full - acc_te_full:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31657967-6780-4762-ab2a-bc70b13c0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.3 — NN on reduced feature set selected by best Decision Tree\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 0) Safety: make sure we have dt_best, matrices Xtr/Xte\n",
    "if 'dt_best' not in globals():\n",
    "    raise RuntimeError(\"dt_best not found. Please run the Task 4 bootstrap cell first.\")\n",
    "prep_dt = dt_best.named_steps[\"prep\"]\n",
    "dt_clf  = dt_best.named_steps[\"clf\"]\n",
    "\n",
    "# If the matrices from 4.2 are missing, build them now\n",
    "if 'prep_for_nn' not in globals():\n",
    "    prep_for_nn = prep_dt\n",
    "if 'Xtr' not in globals() or 'Xte' not in globals():\n",
    "    Xtr = prep_for_nn.transform(X_train)\n",
    "    Xte = prep_for_nn.transform(X_test)\n",
    "    if hasattr(Xtr, \"toarray\"): Xtr = Xtr.toarray()\n",
    "    if hasattr(Xte, \"toarray\"): Xte = Xte.toarray()\n",
    "\n",
    "# 1) Build robust feature names (avoid calling prep_dt.get_feature_names_out())\n",
    "#    - numeric names = original numeric columns\n",
    "#    - categorical names = OHE.get_feature_names_out(cat_original_cols)\n",
    "num_cols, cat_cols = [], []\n",
    "for name, trans, cols in prep_dt.transformers_:\n",
    "    if name == \"num\":\n",
    "        num_cols = list(cols)\n",
    "    elif name == \"cat\":\n",
    "        cat_cols = list(cols)\n",
    "\n",
    "ohe = prep_dt.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "cat_ohe_names = ohe.get_feature_names_out(cat_cols)\n",
    "feat_names = np.array(list(num_cols) + list(cat_ohe_names))\n",
    "\n",
    "# 2) Use DT importances to pick top-K features\n",
    "importances = dt_clf.feature_importances_\n",
    "assert importances.shape[0] == feat_names.shape[0], \\\n",
    "    f\"Length mismatch: importances={importances.shape[0]} vs names={feat_names.shape[0]}\"\n",
    "\n",
    "K = 25  \n",
    "top_idx = np.argsort(importances)[::-1][:K]\n",
    "top_feats = feat_names[top_idx]\n",
    "\n",
    "# 3) Slice reduced matrices\n",
    "Xtr_red = Xtr[:, top_idx]\n",
    "Xte_red = Xte[:, top_idx]\n",
    "\n",
    "print(f\"Reduced inputs = {K}\")\n",
    "print(\"Sample selected features:\", list(top_feats[:10]), \"...\")\n",
    "\n",
    "# 4) GridSearch for NN on reduced set\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "nn_red_gs = GridSearchCV(\n",
    "    estimator=MLPClassifier(random_state=RANDOM_STATE, early_stopping=True, solver=\"adam\"),\n",
    "    param_grid={\n",
    "        \"hidden_layer_sizes\": [(64,), (64,32)],\n",
    "        \"activation\": [\"relu\", \"tanh\"],\n",
    "        \"alpha\": [1e-4, 1e-3],\n",
    "        \"learning_rate_init\": [1e-3, 3e-3],\n",
    "        \"batch_size\": [64, 128],\n",
    "        \"max_iter\": [300]\n",
    "    },\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit=True\n",
    ")\n",
    "nn_red_gs.fit(Xtr_red, y_train)\n",
    "nn_red = nn_red_gs.best_estimator_\n",
    "\n",
    "# 5) Metrics for the report\n",
    "yhat_tr = nn_red.predict(Xtr_red)\n",
    "yhat_te = nn_red.predict(Xte_red)\n",
    "acc_tr_red = accuracy_score(y_train, yhat_tr)\n",
    "acc_te_red = accuracy_score(y_test,  yhat_te)\n",
    "proba_te_red = nn_red.predict_proba(Xte_red)[:,1]\n",
    "auc_te_red = roc_auc_score(y_test, proba_te_red)\n",
    "\n",
    "print(\"Best params (NN reduced):\", nn_red_gs.best_params_)\n",
    "print(f\"NN tuned (reduced) | Train acc={acc_tr_red:.3f}  Test acc={acc_te_red:.3f}  Test AUC={auc_te_red:.3f}\")\n",
    "print(f\"Converged iters (reduced): {nn_red.n_iter_}  (max_iter={nn_red.max_iter}, early_stopping={nn_red.early_stopping})\")\n",
    "print(f\"Overfitting gap (reduced; train - test acc): {acc_tr_red - acc_te_red:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ce27a-b2f0-41c2-9d84-36bef8a57ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.4 — ROC curves (full vs reduced)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# --- 0) Ensure we have preprocessor and split ---\n",
    "if 'dt_best' not in globals():\n",
    "    raise RuntimeError(\"dt_best is missing. Run the Task 4 bootstrap cell first.\")\n",
    "prep_for_nn = dt_best.named_steps['prep']\n",
    "\n",
    "# Build Xtr/Xte if missing\n",
    "if 'Xtr' not in globals() or 'Xte' not in globals():\n",
    "    Xtr = prep_for_nn.transform(X_train)\n",
    "    Xte = prep_for_nn.transform(X_test)\n",
    "    if hasattr(Xtr, \"toarray\"): Xtr = Xtr.toarray()\n",
    "    if hasattr(Xte, \"toarray\"): Xte = Xte.toarray()\n",
    "\n",
    "# --- 1) Ensure nn_full exists\n",
    "if 'nn_full' not in globals():\n",
    "    if 'nn_gs' in globals():\n",
    "        nn_full = nn_gs.best_estimator_\n",
    "    else:\n",
    "        # simple fallback baseline\n",
    "        nn_full = MLPClassifier(hidden_layer_sizes=(64,), activation=\"relu\",\n",
    "                                solver=\"adam\", max_iter=300, early_stopping=True,\n",
    "                                random_state=RANDOM_STATE)\n",
    "        nn_full.fit(Xtr, y_train)\n",
    "\n",
    "# --- 2) Ensure reduced inputs & nn_red exist (rebuild if needed) ---\n",
    "need_red_matrix = ('Xtr_red' not in globals()) or ('Xte_red' not in globals()) or ('top_idx' not in globals())\n",
    "need_nn_red     = ('nn_red' not in globals())\n",
    "\n",
    "if need_red_matrix or need_nn_red:\n",
    "    # Recompute reduced feature indices from dt_best (using OHE names)\n",
    "    prep_dt = dt_best.named_steps[\"prep\"]\n",
    "    dt_clf  = dt_best.named_steps[\"clf\"]\n",
    "\n",
    "    # numeric + cat original cols\n",
    "    num_cols, cat_cols = [], []\n",
    "    for name, trans, cols in prep_dt.transformers_:\n",
    "        if name == \"num\":\n",
    "            num_cols = list(cols)\n",
    "        elif name == \"cat\":\n",
    "            cat_cols = list(cols)\n",
    "\n",
    "    ohe = prep_dt.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    cat_ohe_names = ohe.get_feature_names_out(cat_cols)\n",
    "    feat_names = np.array(list(num_cols) + list(cat_ohe_names))\n",
    "\n",
    "    importances = dt_clf.feature_importances_\n",
    "    assert importances.shape[0] == feat_names.shape[0], \\\n",
    "        f\"Length mismatch: importances={importances.shape[0]} vs names={feat_names.shape[0]}\"\n",
    "\n",
    "    if 'K' not in globals():\n",
    "        K = 25  # same K as in 4.3\n",
    "    top_idx = np.argsort(importances)[::-1][:K]\n",
    "    top_feats = feat_names[top_idx]\n",
    "\n",
    "    # Build reduced matrices\n",
    "    Xtr_red = Xtr[:, top_idx]\n",
    "    Xte_red = Xte[:, top_idx]\n",
    "\n",
    "if need_nn_red:\n",
    "    if 'nn_red_gs' in globals():\n",
    "        nn_red = nn_red_gs.best_estimator_\n",
    "    else:\n",
    "        # simple fallback: small NN on reduced set\n",
    "        nn_red = MLPClassifier(hidden_layer_sizes=(64,), activation=\"relu\",\n",
    "                               solver=\"adam\", max_iter=300, early_stopping=True,\n",
    "                               random_state=RANDOM_STATE)\n",
    "        nn_red.fit(Xtr_red, y_train)\n",
    "\n",
    "# --- 3) Plot ROC for both models ---\n",
    "p_full = nn_full.predict_proba(Xte)[:,1] if hasattr(nn_full, \"predict_proba\") else nn_full.decision_function(Xte)\n",
    "p_red  = nn_red.predict_proba(Xte_red)[:,1] if hasattr(nn_red, \"predict_proba\") else nn_red.decision_function(Xte_red)\n",
    "\n",
    "fpr_f, tpr_f, _ = roc_curve(y_test, p_full)\n",
    "fpr_r, tpr_r, _ = roc_curve(y_test, p_red)\n",
    "auc_f = auc(fpr_f, tpr_f); auc_r = auc(fpr_r, tpr_r)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_f, tpr_f, lw=2, label=f\"NN full (AUC={auc_f:.3f})\")\n",
    "plt.plot(fpr_r, tpr_r, lw=2, label=f\"NN reduced (AUC={auc_r:.3f})\")\n",
    "plt.plot([0,1],[0,1],'k--',lw=1)\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC — Neural Networks\")\n",
    "plt.legend(loc=\"lower right\"); plt.show()\n",
    "\n",
    "# --- 4) Print quick metrics & report snippets ---\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "yhat_tr_full = nn_full.predict(Xtr);    yhat_te_full = nn_full.predict(Xte)\n",
    "yhat_tr_red  = nn_red.predict(Xtr_red); yhat_te_red  = nn_red.predict(Xte_red)\n",
    "\n",
    "acc_tr_full = accuracy_score(y_train, yhat_tr_full)\n",
    "acc_te_full = accuracy_score(y_test,  yhat_te_full)\n",
    "acc_tr_red  = accuracy_score(y_train, yhat_tr_red)\n",
    "acc_te_red  = accuracy_score(y_test,  yhat_te_red)\n",
    "\n",
    "print(\"\\n=== REPORT SNIPPETS ===\")\n",
    "if 'nn_gs' in globals():\n",
    "    print(f\"(i) Full NN best params: {nn_gs.best_params_}\")\n",
    "else:\n",
    "    print(\"(i) Full NN best params: (fallback baseline used)\")\n",
    "print(f\"(ii) Full NN: Train acc={acc_tr_full:.3f}, Test acc={acc_te_full:.3f}, Test AUC={auc_f:.3f}, \"\n",
    "      f\"n_iter={getattr(nn_full,'n_iter_', 'NA')}, early_stopping={getattr(nn_full,'early_stopping','NA')}\")\n",
    "if 'nn_red_gs' in globals():\n",
    "    print(f\"(iii) Reduced NN best params: {nn_red_gs.best_params_}\")\n",
    "else:\n",
    "    print(\"(iii) Reduced NN best params: (fallback baseline used)\")\n",
    "print(f\"(iv) Reduced NN: Train acc={acc_tr_red:.3f}, Test acc={acc_te_red:.3f}, Test AUC={auc_r:.3f}, \"\n",
    "      f\"n_iter={getattr(nn_red,'n_iter_', 'NA')}, early_stopping={getattr(nn_red,'early_stopping','NA')}\")\n",
    "print(f\"(v) Selected inputs (top-{len(top_idx)} by DT):\", list(top_feats[:15]), \"...\")\n",
    "print(f\"(vi) Which is better? {'Reduced' if auc_r>auc_f else 'Full'} model has higher AUC.\")\n",
    "print(\"Interpretation: NN is not easily interpretable; we use DT-selected inputs as a proxy to describe 'kick' tendencies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416255c-0ef6-4ab6-a535-6bd5ac8467ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7929b2-2c46-4436-8220-59173fe5c4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
